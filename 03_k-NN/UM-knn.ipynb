{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uczenie maszynowe\n",
    "## Uczenie poprzez zapamiętywanie (*memory-based learning*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1 - klątwa wielowymiarowości (zjawisko wysokiego stężenia danych *concentration*)\n",
    "Wylosuj dwie obserwacje z wielowymiarowego rozkładu jednorodnego (`np.random.rand`) oraz policz odległość pomiędzy nimi (`np.linalg.norm`). Sporządź wykres odległości pomiędzy dwoma losowymi elementami względem zwiększającej się liczby wymiarów (np. od 2 do 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wraz ze zwiększającą się liczbą wymiarów odległość pomiędzy dwoma losowymi punktami rośnie. Jak myślisz, dlaczego tak jest? Czy jest to efekt spodziewany?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie**\n",
    "Postawmy więc pytanie w inny sposób: mając losowy punkt `x` jak zmienia się stosunek długości najdalszego sąsiada `a` do najbliższego `b`? Spodziewamy się, że $\\frac{||x-a||}{||x-b||}$ powinno być jakąś dużą liczbą, bo gdyby $\\frac{||x-a||}{||x-b||} =1$ oznaczałoby to, że najbliższy punkt w zbiorze jest tak samo daleko jako punkt najdalszy.\n",
    "\n",
    "Powtórz porzedni eksperyment rozszerzając go w trojaki sposób. \n",
    "- Po pierwsze dla każdej liczby wymiarów wykonaj wiele losowań (np. 100) a wynik pokazywany na wykresie uśrednij. \n",
    "- Zamiast losowania dwóch punktów, losuj trzy punkty: `x`, `a`, `b`.\n",
    "- Na wykresie zamiast odległości zwizualizuj stosunek $\\frac{||x-a||}{||x-b||}$ pamiętaj że `a` jest *najdalszym* punktem od x (czyli dalszy niż `b`)\n",
    "\n",
    "*Dla chętnych: Jeśli chcesz to zamiast losować 3 punkty możesz losować cały zbiór danych np. 100-elementowy i w nim szukać najbliższego i najdalszego punktu od pewnej wybranej obserwacji - dostaniesz stablniejsze wyniki*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wraz z rosnącą liczbą wymiarów stosunek odległości zaczyna dążyć do 1!\n",
    "\n",
    "**Zadanie: badanie różnych funkcji odległości**\n",
    "\n",
    "Warto sprawdzić jak na klątwę wielowymiarowości reagują poszczególne miary odległości. Pewną szczególną klasę funkcji odległości stanowi odległość Minkowskiego\n",
    "$$L_m(x-y) = \\left(\\sum_{i=1}^n |x_i-y_i|^m \\right)^{1/m}$$\n",
    "gdzie odlegość $L_2$ to zwykła odległość Euklidesowa, $L_1$ to odległość miejska itd. Każda z tych odległości może być prosto policzona używając `np.linalg.norm` - spójrz do dokumentacji. Uwaga: $m$ potencjalnie może być niecałkowite, w szczególności mniejsze niż 1.\n",
    "\n",
    "Zwizualizuj na jednym wykresie badany stosunek normy $L_1$ i $L_2$, aby jeszcze wyraźniej widzieć różnice w ich działaniu oprócz stosunku odległości najdalszego i najbliższego sąsiada stwórz też wykres różnicy pomiędzy tymi odległościami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Określ wpływ klątwy wielowymiarowości na inne odległości Minkowskiego (nie tylko dla $m=2$ i $m=1$). Czy widzisz jakieś zależności? Czy zarekomendowałbyś którąś z tych odległości do problemów wielowymiarowych?\n",
    "- Przeanalizuj wykres różnych \"okręgów\" o promieniu 1 dla różnych norm. Czy każda z nich spełnia własności normy?\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/0/00/2D_unit_balls.svg)\n",
    "- Czy widzisz jakieś zalety lub wady stosowani metryk $L_2$ i $L_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2 - klątwa wielowymiarowości (zjawisko powstawania koncentratorów - *hubness*)\n",
    "Nawet pracując w dwóch wymiarach z łatwością (?) jesteśmy w stanie podać przykład danych w których $x_1$ jest najbliższym sąsiadem $x_2$ chociaż $x_2$ nie jest najbliższym sąsiadem $x_1$. Pomimo tego, nasze intuicyjne rozumienie klasyfikacji na podstawie podobieństwa oparte jest na założeniu że jak coś jest podobne do drugiego elemnentu to ten drugie element też jest podobny do pierwszego. Spodziewamy się więc, że opisane wcześńiej zjawisko braku symetrii w relacji najbliższego sąsiad nie będzie zbyt częste. Zbadajmy ten efekt biorąc pod uwagę zwiększającą się liczbę wymiarów.\n",
    "\n",
    "Dla podanej liczby wymiarów wylosuj N elementowy zbiorów z rozkładu jednorodnego `np.random.rand`, a następnie dla każdego elementu zlicz dla ilu elementów jest on najbliższym sąsiadem. W celu szukania najbliższych sąsiadów możesz użyć gotową strukturę kd-tree z pakietu sklearn.\n",
    "```\n",
    "tree = KDTree (dane)\n",
    "odleglosci, indeksy_elementow = tree.query(dane których k sasiadow szukamy, k=...)  \n",
    "```\n",
    "Zwróć uwagę, że skoro szukasz najbliższego sąsiada w tym samym zbiorze danych to najbliższym sąsiadem elementu jest tenże element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "def generate(N = 100):\n",
    "    #Parametrem funkcji jest wielkość analizowanego zbiory danych\n",
    "    np.random.seed(10)\n",
    "    results = defaultdict(lambda : np.zeros(N) -1)\n",
    "    #Słownik zawierający zliczenia dla poszczególnych elementów i zbiorów danych\n",
    "    #np. results [i] [j] zawiera liczbę elementów dla których j-ty element jest najbliższym  \n",
    "    #      sąsiadem w zbiorze i-wymiarowym\n",
    "    for i in [2, 3, 5, 10, 20, 100, 1000]: #wymiarowości do przebadania\n",
    "        #TWÓJ KOD TUTAJ\n",
    "\n",
    "    return results\n",
    "\n",
    "r = generate()\n",
    "for i in r.keys():\n",
    "    plt.hist(r[i], label=i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykres zbiorczy może nie wiele powiedzieć - zobacz histogramy dla poszczególnych zbiorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [100]: # wyświetl histogram dla 2D\n",
    "    plt.hist(r[i], label=i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wraz z rosnącą liczbą wymiarów co raz częściej pojawiają się obserwacje - koncentratory. Czyli obserwacje, które są najbliższym sąsiadem do wysokiej liczby elementów. Zwróć uwagę, że takie elementy z pewnością łamią symetrię - mają tylko 1 najbliższego sąsiada samemu będąc najbliższym sąsiadem dla wielu. Dodatkowo wartość klasy decyzyjnej dla takie elementu ma bardzo duże znaczenie dla ogólnej trafności. W przykładowym zbiorze (wyniki są losowe więc mogą się różnić od twoich) 1000 wymiarowym była 1 obserwacja będącą najbliższym sąsiadem do 13 przykładów czyli zmiana etykiety tego przykladu spowodowałaby zmianę klasyfikacji 13\\% instancji w zbiorze!  zbiorze 100-wymiarowym byy 3 instancje które łącznie były najbliższymi sąsiadami do 20\\% instancji. W zbiorach niskowymiarowych jedna obserwacja była najbliższym sąsiadem do maksymalnie 3 elementów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3 - klątwa wymiarowości (rzeczywista liczba wymiarów)\n",
    "Czy jednak zawsze wymiarowość zbioru powoduje takie niepożądane efekty? \n",
    "\n",
    "Wygeneruj zbiór 2-wymiarowy składający się z 1000 obserwacji z rozkładu jednorodnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(1000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Spróbuj sztucznie zwiększać wymiarowość zbioru poprzez dopisywanie kolejnych kolumn wypełnionych zerami. Zrób to w poniższej pętli dla kolejnych wymiarowości i sprawdź jak zmienia się stosunek odległości najdalszego do najbliższego sąsiada wraz ze zwiększającą się wymiarowością."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(2) #Losowy, 2D element do którego będzemy szukać sąsiadów\n",
    "results = {}\n",
    "for i in range(2,1000,5):\n",
    "    Xp = ... #Dodaj zerowe kolumny do X\n",
    "    xp = ... #Dodaj zerowe elementy do x\n",
    "    norms = np.linalg.norm(Xp-xp, axis = 1) # Policz odległości do wszystkich punktów\n",
    "    results[i]=  (np.max(norms)-np.min(norms))/np.min(norms)\n",
    "plt.plot(results.keys(), results.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlaczego tak się dzieje? Spodziewałeś się tego efektu?\n",
    "\n",
    "**Zadanie** Zwiększanie wymiarowości danych poprzez dodawanie zer to wyjątkowo trywialny sposób. Spróbujmy wykonać losową projekcję danych dwuwymiarowych na więcej wymiarów. Jak to zrobić? Spróbujmy to zrobić taką projekcję do 3 wymiarów.\n",
    "\n",
    "Wygenerujmy losową macierz projekcji z rozkładu normalnego. Taka macierz musi mieć wymiary 2 (początkowa wymiarowość) na 3 (wymiarowość po projekcji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proj = np.random.normal(size=(2,3))\n",
    "proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokonanie projekcji to na nasze potrzeby po prostu przemnożnie danych przez macierz projekcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xp = X@proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Zwizualizujmy dane po projekcji (jeśli na wykresie mało widać - powtórz losowanie macierzy projekcji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(Xp[:,0], Xp[:,1], Xp[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wiedząc już jak się przeprowdza projekcję, sprawdź w jaki sposób zachowa się stosunek odległości najdalszego i najbliższego sąsiada dla danych które mają 2 wymiary, ale są przeprojektowane na większą liczbę wymiarów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(2) #Losowy, 2D element do którego będzemy szukać sąsiadów\n",
    "results = {}\n",
    "for i in range(2,1000,5):\n",
    "\n",
    "    Xp = .... #Projekcja X\n",
    "    xp = ...  #Projekcja x\n",
    "    norms = np.linalg.norm(Xp-xp, axis = 1) # Policz odległości do wszystkich punktów\n",
    "    results[i]=  ((np.max(norms)-np.min(norms))/np.min(norms))\n",
    "plt.plot(results.keys(), results.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4\n",
    "Przetestuj wpływ liczby $k$ najbliższych sąsiadów na trafność klasyfikacji dla zbiorów `iris`, `wine` i `breast_cancer`. Dane możesz załadować poprzez wywołanie\n",
    "```\n",
    "X, y = data.load_iris(True)\n",
    "```\n",
    "gdzie `iris` możesz zastąpić inną nazwą zbioru. X to macierz zawierająca cechy (w każdym wierszu inna obserwacja, w każdej kolumnie inna cecha), a y to wartości klasy decyzyjnej dla kolejnych obserwacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as data\n",
    "X, y = data.load_iris(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podzielmy dane na część uczącą i testową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję badającą trafność klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(real, predict):\n",
    "    #Twoj kod tutaj\n",
    "\n",
    "\n",
    "assert accuracy(np.array([0,0,0,1]), np.array([0,0,0,1])) == 1.\n",
    "assert accuracy(np.array([0,0,0,1]), np.array([0,0,1,1])) == 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj ostateczny eksperyment - sprawdzenie trafności klasyfikacji dla każdego $k$ np. od 1 do 20 i narysowanie wykresu. Wykorzystaj gotową implementację klasyfikatora kNN z pakietu sklearn `KNeighborsClassifier` [dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poeksperymentuj z wyżej wymienionymi zbiorami:\n",
    " - czy dla każdego zbioru widać, ze nieparzysta liczba $k$ daje wyższą trafność? \n",
    " - czy wartość $k=1$ jest zawsze najlepsza? Czy są zbiory dla których optymalna wartość $k$ jest bardzo duża np. 31?\n",
    " - czy widzisz jakieś różnice związane z wyborem $k$ dla zbiorów z różną liczbą klas?\n",
    " - czy analizując te zbiory ze względu na ich rozmiar, liczbę wymiarów i najlepsze znalezione $k$ - widzisz zależności o których mówiliśmy na laboratoriach?\n",
    " - spróbuj przeskalować cechy np. poprzez MinMaxScaler - czy widzisz zaletę wynikającą z pracy na ustandaryzowanych danych?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
