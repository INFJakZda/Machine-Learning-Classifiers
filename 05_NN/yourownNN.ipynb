{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe\n",
    "## Implementacja wielowarstwowej sieci neuronowej\n",
    "Wczytaj zbiór danych o irysach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twoim zadaniem będzie implementacja wielowarstwowej sieci neuronowej aktualizowanej algorymem stochastycznego spadku wzdłuż gradientu. Implementacja będzie umożliwiała dowolną architekturę warstwową.\n",
    "\n",
    "Kluczowym elementem naszej implementacji jest ogólna klasa `Neuron` z której dziedziczyć będą wszystkie pozostałe obiekty implementujące neurony np. liniowy, ReLU, softmax itd. Nazwa `Neuron` może być trochę myląca ponieważ w rzeczywistości klasa będze implementowała całą warstwę danego typu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def update(self, gradient, alpha):\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa ma dwie metody:\n",
    "- `forward (x)` zwraca wynik warstwy dla podanego wejścia \n",
    "- `update (gradient, alpha)` funkcja będąca cześcią algorytmu propagacji wstecznej, wykonująca krok algorytmu SGD dla tej konkretnej warstwy. <br>  Funkcja oblicza swój lokalny gradient a następnie korzystając z reguły łańcuchowej (oraz gradientu z kolejnej warstwy który jest na wejściu funkcji) oblicza i zwraca swój gradient. W międzyczasie funkcja aktualizuje parametry warstwy (jeśli je ma) wykonując krok algorytmu SGD z szybkością optymalizacji $\\alpha$.\n",
    "\n",
    "Poniżej możesz prześledzić implementację przykładowego neuronu typu ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RELU(Neuron):\n",
    "    def __init__(self):\n",
    "        self.sm = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.sm = np.maximum(x, 0)\n",
    "        return self.sm\n",
    "    \n",
    "    def update(self, gradient, alpha):\n",
    "        return gradient * (self.sm > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `forward` oblicza funkcję aktywacji:\n",
    "$$\\max(x,0)$$\n",
    "Gdyby wejście było jednowymiarowe to pochodna z tej funkcji to $1$ jeżeli $x>0$ oraz $0$ jeżeli $x<0$. Skoro wejście jest wielowymiarowe to gradient to wektor zer i jedynek dla kolejnych elementów wektora wejściowego $x$. Ponieważ w momencie wstecznej propagacji nie mamu już dostępu do $x$ to funkcja `forward` zapisuje swoje wyjście, aby je później wykorzystać w `update`.\n",
    "\n",
    "Funkcja `update` oblicza swój lokalny gradient (wcześniej opisany, zera i jedynki) oraz przemnaża go (zgodnie z regułą łańcuchową) przez gradient wejściowy. Ponieważ sama nieliniowość nie ma żadnych parametrów to oprócz przekazania obliczonego gradientu funkcji nie robi poza tym nic (normalnie jeszcze by zaktualizowała swoje parametry).\n",
    "\n",
    "Zwróć uwagę, że zwykle przez warstwe ReLU mamy na myśli funkcję liniową + funkcję aktywacji ReLU. Jednak skoro będziemy tak czy tak implementować wektor liniowy to nie ma potrzeby komplikowania naszego obiektu. Po prostu aby uzyskać warstwę ReLU wstawimy do sieci warstwę liniową a potem warstwę funkcji aktywacji ReLU (której implementacje właśnie widziałeś) uzyskując ten sam efekt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj warstwę (tylko aktywacja, bez liniowości) funkcji logistycznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Logistic(Neuron):\n",
    "    def __init__(self):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "    \n",
    "    def update(self, gradient, alpha):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log = Logistic()\n",
    "#Poniższe wywołanie powinno zwrócić\n",
    "#array([0.04742587, 0.5       , 0.73105858, 0.95257413])\n",
    "log.forward(np.array([-3,0,1,3]))\n",
    "#Poniższe wywołanie powinno zwrócić\n",
    "#array([ 0.04517666,  0.        , -0.19661193,  0.02258833])\n",
    "log.update(np.array([1,0,-1, 0.5]), 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sercem naszej implementacji sieci warstwowej jest warstwa liniowa. Będzie to trudniejsza warstwa w implementacji bo ma ona parametry (wagi) które należy zaktualizować w czasie wstecznej propagacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Neuron):\n",
    "    def __init__(self, n_neurons, n_inputs):\n",
    "        \"\"\"\n",
    "        Tworzy warstwę liniową z \"n_neurons\" neuronami oraz \"n_inputs\" wejściami\n",
    "        czyli wejściem do forward będzie wektor o długości n_inputs a wyjściem\n",
    "        wektor o długości n_neurons.\n",
    "        Pamiętaj, że wagi na początku uczenia powinny być losowe\n",
    "        \"\"\"\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "        \n",
    "       \n",
    "    def forward(self,x):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "    \n",
    "    def update(self,gradient , alpha):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin = Linear(2,3)\n",
    "#Zakładając na potrzeby testów, że wagi pierszego neuronu to\n",
    "# 1, 2, 3 oraz zerowy bias\n",
    "# a drugiego neuronu\n",
    "# 0, 1, -1 oraz zerowy bias\n",
    "lin.W = np.array([[1,2,3.], [0,1,-1]])\n",
    "# Poniższe wywołanie powinno zwrócić\n",
    "# [ 8. -1.]\n",
    "print(lin.forward(np.array([0., 1., 2.])))\n",
    "# Poniższe wywołanie powinno zwrócić\n",
    "# [-0.5 -0.8 -1.7]\n",
    "print(lin.update(np.array([-0.5, 0.2]), 0.1))\n",
    "# A po tych operacjach nowe wagi powinny wyglądać następująco:\n",
    "# 1.    2.05  3.1 oraz bias równy 0.05\n",
    "# 0.    0.98 -1.04 oraz bias równy -0.02\n",
    "print(lin.W)\n",
    "print(lin.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnia warstwa do implementacji to wyjściowa warstwa softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMax(Neuron):\n",
    "    def __init__(self):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "    \n",
    "    def update(self, gradient, alpha):\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejnym ważnym elementem jest klasa `NeuralNetwork`, która implementuje nasz klasyfikator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, architecture):\n",
    "        \"\"\"\n",
    "        Jako parametr wejściowy podajemy listę warstw sieci neuronowej \n",
    "        (czyli listę obiektów typu Neuron)\n",
    "        \"\"\"\n",
    "        self.network = architecture\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Funkcja zwraca wyjście NN dla podanego przykładu wejściowego.\n",
    "        Innymi słowy funkcja wykonuje fazę forward propagation\n",
    "        \"\"\"\n",
    "        #TWÓJ KOD TUTAJ\n",
    "        pass\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Dla podanego zbioru danych funkcja wypisuje na ekran wartości:\n",
    "         - trafności klasyfikacji\n",
    "         - funkcji straty: tutaj entropii krzyżowej\n",
    "        Oczywiście funkcja powinna korzystać z predict\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        accuracy = None\n",
    "        #TWÓJ KOD TUTAJ\n",
    "\n",
    "        print('Acc', accuracy, 'Loss:', loss)\n",
    "        \n",
    "    def fit(self, X, y, alpha = 0.003, epoch = 100):\n",
    "        \"\"\"\n",
    "        Funkcja trenująca sieć neuronową\n",
    "        - alpha to szybkość uczenia\n",
    "        - epoch to liczba epok do wykonania\n",
    "        - X, y to zbiór uczący\n",
    "        \"\"\"\n",
    "        for i in range(epoch): #Powtórz ile jest epok\n",
    "            for j in range(X.shape[0]): #Dla każdego indeksu przykładu\n",
    "                z = self.predict(X[j]) #Faza forward\n",
    "                # TWÓJ KOD TUTAJ\n",
    "                # 1) Gradient funkcji celu\n",
    "\n",
    "                # 2) Faza backpropagation\n",
    "\n",
    "            self.calculate_loss(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj brakujące elementy klasy `NeuralNetwork` - radzę uzupełniać od górych funkcji do dolnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Przykładowa architektura sieci neuronowej: \n",
    "- warstwa liniowa 4 wejścia, 5 neuronów\n",
    "- warstwa funkcji aktywacji ReLU\n",
    "- warstwa liniowa 5 wejść (tyle w warstwie poprzedniej jest neuronów) i 3 wyjścia (tyle klas)\n",
    "- warstwa funkcji aktywacji SoftMax\n",
    "\"\"\" \n",
    "network = [Linear(5, 4), RELU(), Linear(3, 5), SoftMax()] \n",
    "nn = NeuralNetwork(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niewytrenowanej sieci trafność powinna być ok. 0.33 (3 klasy) a strata mocno ujemna (np. -500, -1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.calculate_loss(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie z domyślnymi parametrami powinno powodować spadek funkcji straty i co jakiś czas wzrost trafności (do ok. $0.85$). Aby uzyskać lepszy wynik wywołaj komórkę kilka razy, ucząc sieć przez kolejne interacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.fit(X,y) #Tranining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli jesteś w tym miejscu to należą ci się duże gratulacje!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
